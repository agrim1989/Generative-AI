{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4q1Z1oGH4UWFFDqTsdMFs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agrim1989/Generative-AI/blob/main/Mistral_AI_All_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mistralai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "A53RG2bCV2mV",
        "outputId": "207e1b66-bcea-4148-ff1c-845eb99bf30f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mistralai\n",
            "  Downloading mistralai-1.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting eval-type-backport>=0.2.0 (from mistralai)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.28.1)\n",
            "Collecting jsonpath-python>=1.0.6 (from mistralai)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pydantic>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.10.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.8.2)\n",
            "Collecting typing-inspect>=0.9.0 (from mistralai)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->mistralai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->mistralai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->mistralai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->mistralai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->mistralai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.0->mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.0->mistralai) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.0->mistralai) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.9.0->mistralai)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->mistralai) (1.3.1)\n",
            "Downloading mistralai-1.5.0-py3-none-any.whl (271 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.6/271.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, jsonpath-python, eval-type-backport, typing-inspect, mistralai\n",
            "Successfully installed eval-type-backport-0.2.2 jsonpath-python-1.0.6 mistralai-1.5.0 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from mistralai import Mistral\n",
        "\n",
        "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
        "model = \"mistral-large-latest\"\n",
        "\n",
        "client = Mistral(api_key=api_key)"
      ],
      "metadata": {
        "id": "kbHZRpDMXeEG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No streaming"
      ],
      "metadata": {
        "id": "Joy89Q-qWsW1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJlcYs-PV0VJ",
        "outputId": "c7cc253c-8afc-4d0b-ac81-d3f635ffa115"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Determining the \"best\" French cheese can be subjective and depends on personal taste, as France offers a wide variety of exceptional cheeses. However, some French cheeses are world-renowned for their unique flavors and qualities. Here are a few notable ones:\n",
            "\n",
            "1. **Camembert de Normandie**: A soft, creamy cheese from the Normandy region, known for its rich, buttery flavor and edible rind.\n",
            "\n",
            "2. **Brie de Meaux**: Often referred to as the \"King of Cheeses,\" this soft cheese has a creamy interior and a slightly moldy rind. It's produced in the Brie region near Paris.\n",
            "\n",
            "3. **Roquefort**: A strong, tangy blue cheese made from sheep's milk in the south of France. It's one of the world's best-known blue cheeses.\n",
            "\n",
            "4. **Comté**: A hard cheese made from unpasteurized cow's milk in the Franche-Comté region. It has a complex flavor that varies with age.\n",
            "\n",
            "5. **Reblochon**: A soft washed-rind and smear-ripened cheese from the Alps. It's known for its creamy texture and nutty flavor.\n",
            "\n",
            "6. **Époisses**: A pungent, soft cheese from Burgundy. It has a strong smell and a rich, salty flavor.\n",
            "\n",
            "Each of these cheeses has its own unique characteristics, and the \"best\" one can vary greatly depending on individual preferences.\n"
          ]
        }
      ],
      "source": [
        "chat_response = client.chat.complete(\n",
        "    model = model,\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the best French cheese?\",\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(chat_response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With streaming"
      ],
      "metadata": {
        "id": "xX7hjvmcXX_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stream_response = client.chat.stream(\n",
        "    model = model,\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the best French cheese?\",\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "\n",
        "for chunk in stream_response:\n",
        "    print(chunk.data.choices[0].delta.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN-yxjqIXPGI",
        "outputId": "d6ad14b5-dc22-43ff-acbf-4384177f7edf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cho\n",
            "osing\n",
            " the\n",
            " \"\n",
            "best\n",
            "\"\n",
            " French\n",
            " cheese\n",
            " can\n",
            " be\n",
            " quite\n",
            " subject\n",
            "ive\n",
            ",\n",
            " as\n",
            " it\n",
            " depends\n",
            " on\n",
            " personal\n",
            " taste\n",
            ".\n",
            " France\n",
            " is\n",
            " renown\n",
            "ed\n",
            " for\n",
            " its\n",
            " wide\n",
            " variety\n",
            " of\n",
            " che\n",
            "es\n",
            "es\n",
            ",\n",
            " with\n",
            " over\n",
            " \n",
            "4\n",
            "0\n",
            "0\n",
            " different\n",
            " types\n",
            ".\n",
            " Here\n",
            " are\n",
            " a\n",
            " few\n",
            " highly\n",
            " regarded\n",
            " ones\n",
            ":\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1\n",
            ".\n",
            " **\n",
            "Cam\n",
            "ember\n",
            "t\n",
            " de\n",
            " Norm\n",
            "and\n",
            "ie\n",
            "**:\n",
            " A\n",
            " soft\n",
            ",\n",
            " cream\n",
            "y\n",
            ",\n",
            " and\n",
            " surface\n",
            "-\n",
            "rip\n",
            "ened\n",
            " cheese\n",
            " from\n",
            " the\n",
            " Norm\n",
            "andy\n",
            " region\n",
            ".\n",
            " It\n",
            "'\n",
            "s\n",
            " one\n",
            " of\n",
            " the\n",
            " most\n",
            " famous\n",
            " French\n",
            " che\n",
            "es\n",
            "es\n",
            " worldwide\n",
            ".\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "2\n",
            ".\n",
            " **\n",
            "B\n",
            "rie\n",
            " de\n",
            " Me\n",
            "aux\n",
            "**:\n",
            " Often\n",
            " referred\n",
            " to\n",
            " as\n",
            " the\n",
            " \"\n",
            "King\n",
            " of\n",
            " Che\n",
            "es\n",
            "es\n",
            ",\"\n",
            " this\n",
            " soft\n",
            " cheese\n",
            " is\n",
            " known\n",
            " for\n",
            " its\n",
            " rich\n",
            ",\n",
            " cream\n",
            "y\n",
            " interior\n",
            " and\n",
            " blo\n",
            "omy\n",
            " r\n",
            "ind\n",
            ".\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3\n",
            ".\n",
            " **\n",
            "Ro\n",
            "qu\n",
            "ef\n",
            "ort\n",
            "**:\n",
            " A\n",
            " sheep\n",
            " milk\n",
            " blue\n",
            " cheese\n",
            " from\n",
            " the\n",
            " south\n",
            " of\n",
            " France\n",
            ",\n",
            " it\n",
            " has\n",
            " a\n",
            " distinctive\n",
            " taste\n",
            " and\n",
            " p\n",
            "ung\n",
            "ent\n",
            " arom\n",
            "a\n",
            ".\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "4\n",
            ".\n",
            " **\n",
            "Com\n",
            "té\n",
            "**:\n",
            " A\n",
            " hard\n",
            " cheese\n",
            " made\n",
            " from\n",
            " un\n",
            "p\n",
            "aste\n",
            "ur\n",
            "ized\n",
            " cow\n",
            "'\n",
            "s\n",
            " milk\n",
            ",\n",
            " it\n",
            " has\n",
            " a\n",
            " complex\n",
            " flavor\n",
            " profile\n",
            " that\n",
            " varies\n",
            " based\n",
            " on\n",
            " age\n",
            ".\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5\n",
            ".\n",
            " **\n",
            "É\n",
            "po\n",
            "iss\n",
            "es\n",
            "**:\n",
            " A\n",
            " p\n",
            "ung\n",
            "ent\n",
            ",\n",
            " washed\n",
            "-\n",
            "r\n",
            "ind\n",
            " cheese\n",
            " from\n",
            " Burg\n",
            "und\n",
            "y\n",
            ",\n",
            " known\n",
            " for\n",
            " its\n",
            " strong\n",
            " arom\n",
            "a\n",
            " and\n",
            " rich\n",
            ",\n",
            " meat\n",
            "y\n",
            " flavor\n",
            ".\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "6\n",
            ".\n",
            " **\n",
            "Reb\n",
            "lo\n",
            "ch\n",
            "on\n",
            "**:\n",
            " A\n",
            " soft\n",
            " washed\n",
            "-\n",
            "r\n",
            "ind\n",
            " and\n",
            " sm\n",
            "ear\n",
            "-\n",
            "rip\n",
            "ened\n",
            " cheese\n",
            " from\n",
            " the\n",
            " Al\n",
            "ps\n",
            ",\n",
            " often\n",
            " used\n",
            " in\n",
            " the\n",
            " dish\n",
            " T\n",
            "art\n",
            "if\n",
            "lette\n",
            ".\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "7\n",
            ".\n",
            " **\n",
            "Ch\n",
            "è\n",
            "vre\n",
            "**:\n",
            " While\n",
            " not\n",
            " a\n",
            " specific\n",
            " type\n",
            ",\n",
            " French\n",
            " go\n",
            "at\n",
            " che\n",
            "es\n",
            "es\n",
            " are\n",
            " diverse\n",
            " and\n",
            " highly\n",
            " regarded\n",
            ",\n",
            " ranging\n",
            " from\n",
            " fresh\n",
            " and\n",
            " cream\n",
            "y\n",
            " to\n",
            " aged\n",
            " and\n",
            " cr\n",
            "umb\n",
            "ly\n",
            ".\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Each\n",
            " of\n",
            " these\n",
            " che\n",
            "es\n",
            "es\n",
            " offers\n",
            " a\n",
            " unique\n",
            " flavor\n",
            " and\n",
            " texture\n",
            " experience\n",
            ",\n",
            " so\n",
            " the\n",
            " \"\n",
            "best\n",
            "\"\n",
            " one\n",
            " really\n",
            " depends\n",
            " on\n",
            " your\n",
            " personal\n",
            " preference\n",
            ".\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With async"
      ],
      "metadata": {
        "id": "o0p3RKdTXnT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "async def main():\n",
        "    model = \"mistral-tiny\"\n",
        "    response = await client.chat.stream_async(\n",
        "        model=model,\n",
        "        messages=[\n",
        "             {\n",
        "                  \"role\": \"user\",\n",
        "                  \"content\": \"Who is the best French painter? Answer in JSON.\",\n",
        "              },\n",
        "        ],\n",
        "    )\n",
        "    async for chunk in response:\n",
        "        if chunk.data.choices[0].delta.content is not None:\n",
        "            print(chunk.data.choices[0].delta.content, end=\"\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "_Q_CJ0TAXilx",
        "outputId": "4d80e598-d0da-498c-d269-77afde873e07"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "asyncio.run() cannot be called from a running event loop",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-35b5a24673cc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# fail fast with short traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    187\u001b[0m             \"asyncio.run() cannot be called from a running event loop\")\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passing an Image URL"
      ],
      "metadata": {
        "id": "NmsMoWxZYGOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify model\n",
        "model = \"pixtral-12b-2409\"\n",
        "\n",
        "# Define the messages for the chat\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": \"What's in this image?\"\n",
        "            },\n",
        "            {\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": \"https://tripfixers.com/wp-content/uploads/2019/11/eiffel-tower-with-snow.jpeg\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Get the chat response\n",
        "chat_response = client.chat.complete(\n",
        "    model=model,\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "# Print the content of the response\n",
        "print(chat_response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FjrklKqX4ZH",
        "outputId": "2c788d56-3f13-43dd-9590-8f59d4dbcc09"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The image showcases a picturesque winter scene with the Eiffel Tower prominently featured in the background. The Eiffel Tower is partially obscured by snow-covered trees and bushes. The ground is blanketed in snow, and there are bare trees with snow resting on their branches. A classic street lamp is visible in the foreground, adding to the charm of the scene. The overall atmosphere is serene and tranquil, with the snow creating a soft, white blanket over the landscape.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passing a Base64 Encoded Image\n"
      ],
      "metadata": {
        "id": "-X7GurJ2YQPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import requests\n",
        "import os\n",
        "from mistralai import Mistral\n",
        "\n",
        "def encode_image(image_path):\n",
        "    \"\"\"Encode the image to base64.\"\"\"\n",
        "    try:\n",
        "        with open(image_path, \"rb\") as image_file:\n",
        "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file {image_path} was not found.\")\n",
        "        return None\n",
        "    except Exception as e:  # Added general exception handling\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Path to your image\n",
        "image_path = \"/abhikhaled.png\"\n",
        "\n",
        "# Getting the base64 string\n",
        "base64_image = encode_image(image_path)\n",
        "\n",
        "# Retrieve the API key from environment variables\n",
        "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
        "\n",
        "# Specify model\n",
        "model = \"pixtral-12b-2409\"\n",
        "\n",
        "# Initialize the Mistral client\n",
        "client = Mistral(api_key=api_key)\n",
        "\n",
        "# Define the messages for the chat\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": \"What's in this image?\"\n",
        "            },\n",
        "            {\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Get the chat response\n",
        "chat_response = client.chat.complete(\n",
        "    model=model,\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "# Print the content of the response\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQw3GLkzYLCQ",
        "outputId": "828d948e-753d-48bf-dd98-9c6d33d3a1f7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The image appears to depict a surgical procedure. There is a close-up view of a surgical site with visible tissue and blood. A surgical instrument, possibly a clamp or forceps, is being used to manipulate or hold the tissue. The surrounding area shows the patient's skin, which has visible blood vessels and some signs of surgical manipulation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import requests\n",
        "import os\n",
        "from mistralai import Mistral\n",
        "\n",
        "def encode_image(image_path):\n",
        "    \"\"\"Encode the image to base64.\"\"\"\n",
        "    try:\n",
        "        with open(image_path, \"rb\") as image_file:\n",
        "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file {image_path} was not found.\")\n",
        "        return None\n",
        "    except Exception as e:  # Added general exception handling\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Path to your image\n",
        "image_path = \"/abhikhaled.png\"\n",
        "\n",
        "# Getting the base64 string\n",
        "base64_image = encode_image(image_path)\n",
        "\n",
        "# Retrieve the API key from environment variables\n",
        "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
        "\n",
        "# Specify model\n",
        "model = \"pixtral-12b-2409\"\n",
        "\n",
        "# Initialize the Mistral client\n",
        "client = Mistral(api_key=api_key)\n",
        "\n",
        "# Define the messages for the chat\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": \"Generate an image where a person walking on a trail during a sunset\"\n",
        "            }]\n",
        "\n",
        "    }\n",
        "]\n",
        "\n",
        "# Get the chat response\n",
        "chat_response = client.chat.complete(\n",
        "    model=model,\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "# Print the content of the response\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMh_qQREYTd_",
        "outputId": "363c748e-fc5c-4d85-a77e-462f3c5e3a89"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm unable to generate images directly, but I can certainly help you visualize or describe the scene you're looking for. Imagine this:\n",
            "\n",
            "A person is walking along a winding trail that cuts through a lush, green forest. The trail is covered with a soft layer of pine needles and small pebbles, and it gently curves to the right, disappearing into the distance. The person is dressed in casual hiking attire, perhaps wearing a backpack and a light jacket, enjoying the cool evening air.\n",
            "\n",
            "As they walk, the sun is setting, casting a warm, golden glow through the trees. The sky is painted with hues of orange, pink, and purple, creating a stunning backdrop. Long shadows stretch out across the forest floor, and the sunlight filtering through the leaves creates a dappled pattern on the trail.\n",
            "\n",
            "The person appears to be in a moment of peaceful reflection, taking in the beauty of nature around them. The scene is serene and tranquil, capturing the essence of a perfect sunset hike.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8rPYt6JfZ5hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code generation\n"
      ],
      "metadata": {
        "id": "CasR_7UzarFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 1: Fill in the middle"
      ],
      "metadata": {
        "id": "y7gep0iza7yR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from mistralai import Mistral\n",
        "\n",
        "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
        "client = Mistral(api_key=api_key)\n",
        "\n",
        "model = \"codestral-latest\"\n",
        "prompt = \"def fibonacci(n: int):\"\n",
        "suffix = \"n = int(input('Enter a number: '))\\nprint(fibonacci(n))\"\n",
        "\n",
        "response = client.fim.complete(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    suffix=suffix,\n",
        "    temperature=0,\n",
        "    top_p=1,\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"\"\"\n",
        "{prompt}\n",
        "{response.choices[0].message.content}\n",
        "{suffix}\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6tcoE0gaJfy",
        "outputId": "d34669fd-f4ea-4f95-ceb7-4cac96dcdab4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "def fibonacci(n: int):\n",
            "\n",
            "    if n == 0:\n",
            "        return 0\n",
            "    elif n == 1:\n",
            "        return 1\n",
            "    else:\n",
            "        return fibonacci(n-1) + fibonacci(n-2)\n",
            "\n",
            "\n",
            "n = int(input('Enter a number: '))\n",
            "print(fibonacci(n))\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 2: Completion"
      ],
      "metadata": {
        "id": "SSdtLNv6bCye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from mistralai import Mistral\n",
        "\n",
        "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
        "client = Mistral(api_key=api_key)\n",
        "\n",
        "model = \"codestral-latest\"\n",
        "prompt = \"def is_odd(n): \\n return n % 2 == 1 \\ndef test_is_odd():\"\n",
        "\n",
        "response = client.fim.complete(model=model, prompt=prompt, temperature=0, top_p=1)\n",
        "\n",
        "print(\n",
        "    f\"\"\"\n",
        "{prompt}\n",
        "{response.choices[0].message.content}\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VqRJKRwaKd7",
        "outputId": "7b51fd6e-79b0-4abc-8dfd-a3b4e2be0a6a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "def is_odd(n): \n",
            " return n % 2 == 1 \n",
            "def test_is_odd():\n",
            "\n",
            " if is_odd(17):\n",
            "  print('17 is odd.')\n",
            " else:\n",
            "  print('17 is not odd.')\n",
            " if is_odd(-17):\n",
            "  print('-17 is odd.')\n",
            " else:\n",
            "  print('-17 is not odd.')\n",
            " if is_odd(18):\n",
            "  print('18 is odd.')\n",
            " else:\n",
            "  print('18 is not odd.')\n",
            "test_is_odd()\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 3: Stop tokens"
      ],
      "metadata": {
        "id": "4hbUNcsubNpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from mistralai import Mistral\n",
        "\n",
        "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
        "client = Mistral(api_key=api_key)\n",
        "\n",
        "model = \"codestral-latest\"\n",
        "prompt = \"def is_odd(n): \\n return n % 2 == 1 \\ndef test_is_odd():\"\n",
        "suffix = \"n = int(input('Enter a number: '))\\nprint(fibonacci(n))\"\n",
        "\n",
        "response = client.fim.complete(\n",
        "    model=model, prompt=prompt, suffix=suffix, temperature=0, top_p=1, stop=[\"\\n\\n\"]\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"\"\"\n",
        "{prompt}\n",
        "{response.choices[0].message.content}\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hJktQ0tbIn0",
        "outputId": "0f2f0384-49fb-473f-dcfa-9f3a8eca2b07"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "def is_odd(n): \n",
            " return n % 2 == 1 \n",
            "def test_is_odd():\n",
            "\n",
            " assert is_odd(1) == True\n",
            " assert is_odd(2) == False\n",
            " assert is_odd(3) == True\n",
            " assert is_odd(4) == False\n",
            " assert is_odd(5) == True\n",
            " print('Yay!')\n",
            "test_is_odd()\n",
            "def fibonacci(n):\n",
            " if n == 0:\n",
            "  return 0\n",
            " elif n == 1:\n",
            "  return 1\n",
            " else:\n",
            "  return fibonacci(n-1) + fibonacci(n-2)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Instruct endpoint"
      ],
      "metadata": {
        "id": "J6P-O9w4ba74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from mistralai import Mistral\n",
        "\n",
        "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
        "client = Mistral(api_key=api_key)\n",
        "\n",
        "model = \"codestral-latest\"\n",
        "message = [{\"role\": \"user\", \"content\": \"Write a function for fibonacci\"}]\n",
        "chat_response = client.chat.complete(\n",
        "    model = model,\n",
        "    messages = message\n",
        ")\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjrMIbC4bVxQ",
        "outputId": "67968c28-fdde-4e3b-def4-1f980eaa9d83"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Certainly! The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones, usually starting with 0 and 1. Here is a simple implementation of a function to generate the Fibonacci sequence in Python:\n",
            "\n",
            "### Iterative Approach\n",
            "```python\n",
            "def fibonacci(n):\n",
            "    if n <= 0:\n",
            "        return []\n",
            "    elif n == 1:\n",
            "        return [0]\n",
            "    elif n == 2:\n",
            "        return [0, 1]\n",
            "\n",
            "    fib_sequence = [0, 1]\n",
            "    for i in range(2, n):\n",
            "        next_value = fib_sequence[-1] + fib_sequence[-2]\n",
            "        fib_sequence.append(next_value)\n",
            "\n",
            "    return fib_sequence\n",
            "\n",
            "# Example usage:\n",
            "print(fibonacci(10))  # Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n",
            "```\n",
            "\n",
            "### Recursive Approach\n",
            "```python\n",
            "def fibonacci_recursive(n, computed={0: 0, 1: 1}):\n",
            "    if n in computed:\n",
            "        return computed[n]\n",
            "\n",
            "    computed[n] = fibonacci_recursive(n - 1, computed) + fibonacci_recursive(n - 2, computed)\n",
            "    return computed[n]\n",
            "\n",
            "def fibonacci(n):\n",
            "    if n <= 0:\n",
            "        return []\n",
            "    return [fibonacci_recursive(i) for i in range(n)]\n",
            "\n",
            "# Example usage:\n",
            "print(fibonacci(10))  # Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n",
            "```\n",
            "\n",
            "### Explanation:\n",
            "1. **Iterative Approach**:\n",
            "   - This method uses a loop to generate the Fibonacci sequence.\n",
            "   - It starts with the first two numbers `[0, 1]` and iteratively adds the sum of the last two numbers to the sequence.\n",
            "   - This approach is efficient and avoids the overhead of recursive calls.\n",
            "\n",
            "2. **Recursive Approach**:\n",
            "   - This method uses a recursive function to calculate the Fibonacci number.\n",
            "   - It uses memoization (a dictionary `computed`) to store already computed values to avoid redundant calculations.\n",
            "   - The `fibonacci` function generates the sequence by calling the recursive function for each index up to `n`.\n",
            "\n",
            "Both approaches will give you the Fibonacci sequence up to the `n`-th term. Choose the one that best fits your needs based on performance and readability.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Codestral Mamba\n"
      ],
      "metadata": {
        "id": "owdT94GVbjfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from mistralai import Mistral\n",
        "\n",
        "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
        "\n",
        "client = Mistral(api_key=api_key)\n",
        "\n",
        "model = \"codestral-mamba-latest\"\n",
        "\n",
        "message = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Write a function for fibonacci\"\n",
        "    }\n",
        "]\n",
        "\n",
        "chat_response = client.chat.complete(\n",
        "    model=model,\n",
        "    messages=message\n",
        ")\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN3vcT5bbc0R",
        "outputId": "6689dcd1-b577-4b72-874e-2ffee3daec2b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here's a simple example of a function that generates Fibonacci numbers in Python:\n",
            "\n",
            "```python\n",
            "def fibonacci(n):\n",
            "    fib_sequence = [0, 1]\n",
            "    while len(fib_sequence) < n:\n",
            "        fib_sequence.append(fib_sequence[-1] + fib_sequence[-2])\n",
            "    return fib_sequence[:n]\n",
            "```\n",
            "\n",
            "This function generates a list of the first `n` numbers in the Fibonacci sequence. The Fibonacci sequence starts with 0 and 1, and each subsequent number is the sum of the previous two.\n",
            "\n",
            "You can use this function like this:\n",
            "\n",
            "```python\n",
            "print(fibonacci(10))  # Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Integration with LangChain"
      ],
      "metadata": {
        "id": "0rj5jLGrfqc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain langchain-mistralai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DpUD3hkftgk",
        "outputId": "b97ad880-167b-4287-f588-24d0bca20bc6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.18)\n",
            "Collecting langchain-mistralai\n",
            "  Downloading langchain_mistralai-0.2.6-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.35)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: httpx<1,>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langchain-mistralai) (0.28.1)\n",
            "Collecting httpx-sse<1,>=0.3.1 (from langchain-mistralai)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from langchain-mistralai) (0.21.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.2->langchain-mistralai) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain) (4.12.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<1,>=0.15.1->langchain-mistralai) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (2024.10.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (4.67.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.25.2->langchain-mistralai) (1.3.1)\n",
            "Downloading langchain_mistralai-0.2.6-py3-none-any.whl (15 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Installing collected packages: httpx-sse, langchain-mistralai\n",
            "Successfully installed httpx-sse-0.4.0 langchain-mistralai-0.2.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure to install `langchain` and `langchain-mistralai` in your Python environment\n",
        "\n",
        "import os\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
        "mistral_model = \"codestral-latest\"\n",
        "llm = ChatMistralAI(model=mistral_model, temperature=0, api_key=api_key)\n",
        "response = llm.invoke([(\"user\", \"Write a function for fibonacci\")])"
      ],
      "metadata": {
        "id": "X7FzsHf8bh5r"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "eokviDF6fxqD",
        "outputId": "f361a6b5-f057-4716-8b26-8fcd4e85592e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Certainly! The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones, usually starting with 0 and 1. Here is a simple implementation of a function to generate the Fibonacci sequence in Python:\\n\\n```python\\ndef fibonacci(n):\\n    \"\"\"\\n    Generate a list containing the Fibonacci sequence up to the nth number.\\n\\n    :param n: The number of terms in the Fibonacci sequence to generate.\\n    :return: A list containing the Fibonacci sequence.\\n    \"\"\"\\n    if n <= 0:\\n        return []\\n    elif n == 1:\\n        return [0]\\n    elif n == 2:\\n        return [0, 1]\\n\\n    fib_sequence = [0, 1]\\n    for i in range(2, n):\\n        next_value = fib_sequence[-1] + fib_sequence[-2]\\n        fib_sequence.append(next_value)\\n\\n    return fib_sequence\\n\\n# Example usage:\\nn = 10\\nprint(fibonacci(n))\\n```\\n\\nThis function `fibonacci` takes an integer `n` as an argument and returns a list containing the first `n` numbers in the Fibonacci sequence. Here\\'s a breakdown of how it works:\\n\\n1. If `n` is less than or equal to 0, it returns an empty list.\\n2. If `n` is 1, it returns a list containing only the first Fibonacci number `[0]`.\\n3. If `n` is 2, it returns a list containing the first two Fibonacci numbers `[0, 1]`.\\n4. For `n` greater than 2, it initializes the list with the first two Fibonacci numbers `[0, 1]` and then iteratively calculates the next Fibonacci number by summing the last two numbers in the list, appending the result to the list until the list contains `n` numbers.\\n\\nYou can call this function with any positive integer to get the corresponding Fibonacci sequence.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Integration with LlamaIndex"
      ],
      "metadata": {
        "id": "XKKNsXnjgD8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index llama-index-llms-mistralai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyuDtSC3gLqc",
        "outputId": "4340983a-e096-41a1-aa59-6dfe13880c4b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.12.19-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting llama-index-llms-mistralai\n",
            "  Downloading llama_index_llms_mistralai-0.3.3-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl.metadata (727 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.13.0,>=0.12.19 (from llama-index)\n",
            "  Downloading llama_index_core-0.12.19-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.3.20-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.4.5-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: mistralai>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-mistralai) (1.5.0)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.61.1)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.19->llama-index) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (3.11.12)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.19->llama-index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.2.18)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.19->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.19->llama-index)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (2.10.6)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (9.0.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.19->llama-index)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.17.2)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.8 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.12-py3-none-any.whl.metadata (851 bytes)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading pypdf-5.3.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from mistralai>=1.0.0->llama-index-llms-mistralai) (0.2.2)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from mistralai>=1.0.0->llama-index-llms-mistralai) (1.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai>=1.0.0->llama-index-llms-mistralai) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.18.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.19->llama-index) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.19->llama-index) (0.14.0)\n",
            "Collecting llama-cloud-services>=0.6.1 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->mistralai>=1.0.0->llama-index-llms-mistralai) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.0.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.19->llama-index)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.1->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.19->llama-index) (24.2)\n",
            "Downloading llama_index-0.12.19-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_llms_mistralai-0.3.3-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_core-0.12.19-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.7-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_llms_openai-0.3.20-py3-none-any.whl (15 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.5-py3-none-any.whl (39 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_cloud-0.1.12-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.6.1-py3-none-any.whl (4.8 kB)\n",
            "Downloading pypdf-5.3.0-py3-none-any.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading llama_cloud_services-0.6.1-py3-none-any.whl (22 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, python-dotenv, pypdf, marshmallow, tiktoken, dataclasses-json, llama-index-core, llama-cloud, llama-index-readers-file, llama-index-llms-openai, llama-index-llms-mistralai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 llama-cloud-0.1.12 llama-cloud-services-0.6.1 llama-index-0.12.19 llama-index-agent-openai-0.4.6 llama-index-cli-0.4.0 llama-index-core-0.12.19 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.7 llama-index-llms-mistralai-0.3.3 llama-index-llms-openai-0.3.20 llama-index-multi-modal-llms-openai-0.4.3 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.5 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.1 marshmallow-3.26.1 pypdf-5.3.0 python-dotenv-1.0.1 striprtf-0.0.26 tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure to install `llama-index` and `llama-index-llms-mistralai` in your Python enviornment\n",
        "\n",
        "import os\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.llms.mistralai import MistralAI\n",
        "\n",
        "api_key =  os.environ[\"MISTRAL_API_KEY\"]\n",
        "mistral_model = \"codestral-latest\"\n",
        "messages = [\n",
        "    ChatMessage(role=\"user\", content=\"Write a function for fibonacci\"),\n",
        "]\n",
        "MistralAI(api_key=api_key, model=mistral_model).chat(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVtPP9eIf7XJ",
        "outputId": "e50aa79c-72a9-4c73-827b-efc0ff9729af"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='Certainly! The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones, usually starting with 0 and 1. Here is a simple implementation of a function to generate the Fibonacci sequence in Python:\\n\\n### Iterative Approach\\nThis approach uses a loop to generate the Fibonacci sequence up to a specified number of terms.\\n\\n```python\\ndef fibonacci_iterative(n):\\n    if n <= 0:\\n        return []\\n    elif n == 1:\\n        return [0]\\n    elif n == 2:\\n        return [0, 1]\\n\\n    fib_sequence = [0, 1]\\n    for i in range(2, n):\\n        next_value = fib_sequence[-1] + fib_sequence[-2]\\n        fib_sequence.append(next_value)\\n\\n    return fib_sequence\\n\\n# Example usage:\\nprint(fibonacci_iterative(10))  # Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\\n```\\n\\n### Recursive Approach\\nThis approach uses recursion to generate the Fibonacci sequence. Note that this method is not efficient for large values of `n` due to repeated calculations.\\n\\n```python\\ndef fibonacci_recursive(n):\\n    if n <= 0:\\n        return []\\n    elif n == 1:\\n        return [0]\\n    elif n == 2:\\n        return [0, 1]\\n\\n    def fib_recursive_helper(n):\\n        if n == 0:\\n            return 0\\n        elif n == 1:\\n            return 1\\n        else:\\n            return fib_recursive_helper(n - 1) + fib_recursive_helper(n - 2)\\n\\n    fib_sequence = [fib_recursive_helper(i) for i in range(n)]\\n    return fib_sequence\\n\\n# Example usage:\\nprint(fibonacci_recursive(10))  # Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\\n```\\n\\n### Memoization Approach\\nThis approach uses memoization to optimize the recursive method by storing previously computed values.\\n\\n```python\\ndef fibonacci_memoization(n):\\n    memo = {}\\n\\n    def fib_memo_helper(n):\\n        if n in memo:\\n            return memo[n]\\n        if n <= 0:\\n            return 0\\n        elif n == 1:\\n            return 1\\n')]), raw={'id': '417aba6d1f2e403aa3128d45d525baaa', 'object': 'chat.completion', 'model': 'codestral-latest', 'usage': UsageInfo(prompt_tokens=9, completion_tokens=512, total_tokens=521), 'created': 1739856101, 'choices': [ChatCompletionChoice(index=0, message=AssistantMessage(content='Certainly! The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones, usually starting with 0 and 1. Here is a simple implementation of a function to generate the Fibonacci sequence in Python:\\n\\n### Iterative Approach\\nThis approach uses a loop to generate the Fibonacci sequence up to a specified number of terms.\\n\\n```python\\ndef fibonacci_iterative(n):\\n    if n <= 0:\\n        return []\\n    elif n == 1:\\n        return [0]\\n    elif n == 2:\\n        return [0, 1]\\n\\n    fib_sequence = [0, 1]\\n    for i in range(2, n):\\n        next_value = fib_sequence[-1] + fib_sequence[-2]\\n        fib_sequence.append(next_value)\\n\\n    return fib_sequence\\n\\n# Example usage:\\nprint(fibonacci_iterative(10))  # Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\\n```\\n\\n### Recursive Approach\\nThis approach uses recursion to generate the Fibonacci sequence. Note that this method is not efficient for large values of `n` due to repeated calculations.\\n\\n```python\\ndef fibonacci_recursive(n):\\n    if n <= 0:\\n        return []\\n    elif n == 1:\\n        return [0]\\n    elif n == 2:\\n        return [0, 1]\\n\\n    def fib_recursive_helper(n):\\n        if n == 0:\\n            return 0\\n        elif n == 1:\\n            return 1\\n        else:\\n            return fib_recursive_helper(n - 1) + fib_recursive_helper(n - 2)\\n\\n    fib_sequence = [fib_recursive_helper(i) for i in range(n)]\\n    return fib_sequence\\n\\n# Example usage:\\nprint(fibonacci_recursive(10))  # Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\\n```\\n\\n### Memoization Approach\\nThis approach uses memoization to optimize the recursive method by storing previously computed values.\\n\\n```python\\ndef fibonacci_memoization(n):\\n    memo = {}\\n\\n    def fib_memo_helper(n):\\n        if n in memo:\\n            return memo[n]\\n        if n <= 0:\\n            return 0\\n        elif n == 1:\\n            return 1\\n', tool_calls=None, prefix=False, role='assistant'), finish_reason='length')]}, delta=None, logprobs=None, additional_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ykJbsT_ogQ_e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}